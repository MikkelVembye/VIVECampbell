<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Full-text screening with OpenAI API models — ftscreen • VIVECampbell</title><!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Full-text screening with OpenAI API models — ftscreen"><meta name="description" content='Performs full-text screening with OpenAI Assistants API models over local documents
(PDF, TXT, DOCX, etc.). You can supply either a single protocol file or one/many
direct prompts; the function can repeat questions (reps) to assess answer
consistency. Native (structured) function calling is used to standardize outputs.
A two-phase workflow is used:
Supplementary detection (precomputation): For every unique input file a
short, standalone assistant run is executed that ONLY calls the
supplementary_check tool. Its single yes/no outcome is normalized
("yes"/"no"/NA) and cached.
Screening runs: Each (file × prompt × repetition × model) run reuses the
cached supplementary value. The main assistant is created WITHOUT the
supplementary_check tool. This prevents drift, saves tokens,
and ensures a single authoritative value per file.


Fallback: If the precomputation fails for a file (result = NA), the screening
runs for that file automatically include the supplementary_check tool and
allow the model to call it inline (once per run) so a value can still be
produced.'><meta property="og:description" content='Performs full-text screening with OpenAI Assistants API models over local documents
(PDF, TXT, DOCX, etc.). You can supply either a single protocol file or one/many
direct prompts; the function can repeat questions (reps) to assess answer
consistency. Native (structured) function calling is used to standardize outputs.
A two-phase workflow is used:
Supplementary detection (precomputation): For every unique input file a
short, standalone assistant run is executed that ONLY calls the
supplementary_check tool. Its single yes/no outcome is normalized
("yes"/"no"/NA) and cached.
Screening runs: Each (file × prompt × repetition × model) run reuses the
cached supplementary value. The main assistant is created WITHOUT the
supplementary_check tool. This prevents drift, saves tokens,
and ensures a single authoritative value per file.


Fallback: If the precomputation fails for a file (result = NA), the screening
runs for that file automatically include the supplementary_check tool and
allow the model to call it inline (once per run) so a value can still be
produced.'></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">VIVECampbell</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/VIVECampbell.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>VIVE specific content</h6></li>
    <li><a class="dropdown-item" href="../articles/Github.html">Forbind GitHub og RStudio plus adgang til Adobe Pro</a></li>
    <li><a class="dropdown-item" href="../articles/meta-analysis-in-R.html">Kom godt i gang med metaanalyse i R</a></li>
    <li><a class="dropdown-item" href="../articles/openai.html">VIVECampbell OpenAI konto</a></li>
    <li><a class="dropdown-item" href="../articles/samle-flere-ris-filer.html">Samle mange ris-filer til en (eller flere) fil(er) i R</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>EPPI tips and manuals</h6></li>
    <li><a class="dropdown-item" href="../articles/eppi-tips.html">Tips og tricks til EPPI</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Effect size calculation</h6></li>
    <li><a class="dropdown-item" href="../articles/Entering-data.html">Indtastning af data, effektstørrelsesudregning og cluster bias justering, når der kun er clustering i en gruppe</a></li>
    <li><a class="dropdown-item" href="../articles/Pooling-subgroups.html">Pooling across (multiple) subgroups</a></li>
    <li><a class="dropdown-item" href="../articles/ancova-puzzler.html">An ANCOVA puzzler</a></li>
    <li><a class="dropdown-item" href="../articles/es-calc-uden-means-og-sds.html">Effektstørrelsesudregning uden means og SDs (med eksempler)</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../articles/index.html">More articles...</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/MikkelVembye/VIVECampbell/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Full-text screening with OpenAI API models</h1>
      <small class="dont-index">Source: <a href="https://github.com/MikkelVembye/VIVECampbell/blob/main/R/ftscreen.r" class="external-link"><code>R/ftscreen.r</code></a></small>
      <div class="d-none name"><code>ftscreen.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Performs full-text screening with OpenAI Assistants API models over local documents
(PDF, TXT, DOCX, etc.). You can supply either a single protocol file or one/many
direct prompts; the function can repeat questions (<code>reps</code>) to assess answer
consistency. Native (structured) function calling is used to standardize outputs.</p>
<p>A two-phase workflow is used:</p><ol><li><p>Supplementary detection (precomputation): For every unique input file a
short, standalone assistant run is executed that ONLY calls the
<code>supplementary_check</code> tool. Its single yes/no outcome is normalized
("yes"/"no"/NA) and cached.</p></li>
<li><p>Screening runs: Each (file × prompt × repetition × model) run reuses the
cached supplementary value. The main assistant is created WITHOUT the
<code>supplementary_check</code> tool. This prevents drift, saves tokens,
and ensures a single authoritative value per file.</p></li>
</ol><p>Fallback: If the precomputation fails for a file (result = NA), the screening
runs for that file automatically include the <code>supplementary_check</code> tool and
allow the model to call it inline (once per run) so a value can still be
produced.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">ftscreen</span><span class="op">(</span></span>
<span>  <span class="va">file_path</span>,</span>
<span>  prompt <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  protocol_file_path <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  api_key <span class="op">=</span> <span class="fu">AIscreenR</span><span class="fu">::</span><span class="fu"><a href="https://mikkelvembye.github.io/AIscreenR/reference/get_api_key.html" class="external-link">get_api_key</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  <span class="va">vector_stores_name</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>,</span>
<span>  top_p <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span>  decision_description <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  assistant_name <span class="op">=</span> <span class="st">"file assistant screening"</span>,</span>
<span>  assistant_description <span class="op">=</span></span>
<span>    <span class="st">"Function-calling assistant for full-text screening that MUST return answers exclusively via a single required tool/function call (no plain-text replies). Reviews a file and decides inclusion/exclusion/uncertain per a protocol or prompt."</span>,</span>
<span>  assistant_instructions <span class="op">=</span></span>
<span>    <span class="st">"You are a function-calling assistant for full-text screening.\n\nCRITICAL REQUIREMENTS:\n- Always return your answer by emitting EXACTLY ONE tool/function call required for the task.\n- Do NOT output plain text answers; do NOT call any other tools.\n- If the prompt/instructions indicate supplementary presence is precomputed, do NOT call supplementary_check.\n\nTASK:\n- Review the attached file strictly against the provided protocol or prompt.\n- Decide include (1), exclude (0), or unclear (1.1). When detailed mode is enabled, provide the rationale via the tool fields.\n\nOUTPUT:\n- Only the single function call with the appropriate arguments."</span>,</span>
<span>  messages <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  reps <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  max_tries <span class="op">=</span> <span class="fl">16</span>,</span>
<span>  time_info <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  token_info <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  max_seconds <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  is_transient <span class="op">=</span> <span class="va">.gpt_is_transient</span>,</span>
<span>  backoff <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  after <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  rpm <span class="op">=</span> <span class="fl">10000</span>,</span>
<span>  seed_par <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  progress <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  incl_cutoff_upper <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  incl_cutoff_lower <span class="op">=</span> <span class="fl">0.4</span>,</span>
<span>  force <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  sleep_time <span class="op">=</span> <span class="fl">8</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-file-path">file_path<a class="anchor" aria-label="anchor" href="#arg-file-path"></a></dt>
<dd><p>A character vector of file paths or directories to be screened. Directories will be processed recursively, with files in sub-directories being combined into a single document for screening.</p></dd>


<dt id="arg-prompt">prompt<a class="anchor" aria-label="anchor" href="#arg-prompt"></a></dt>
<dd><p>A character string containing the screening prompt. Required if <code>protocol_file_path</code> is not provided.</p></dd>


<dt id="arg-protocol-file-path">protocol_file_path<a class="anchor" aria-label="anchor" href="#arg-protocol-file-path"></a></dt>
<dd><p>Path to a file (.txt, .md, .pdf, .docx) containing the screening protocol.</p></dd>


<dt id="arg-api-key">api_key<a class="anchor" aria-label="anchor" href="#arg-api-key"></a></dt>
<dd><p>Your OpenAI API key. Defaults to <code>get_api_key()</code>. Imported from AIscreenR package.</p></dd>


<dt id="arg-vector-stores-name">vector_stores_name<a class="anchor" aria-label="anchor" href="#arg-vector-stores-name"></a></dt>
<dd><p>A name for the OpenAI vector store to be created for the screening session.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Character string with the name of the completion model. Can take
multiple OpenAI models. Default = <code>"gpt-4o-mini"</code>.
Find available models at <a href="https://platform.openai.com/docs/models" class="external-link">https://platform.openai.com/docs/models</a>.</p></dd>


<dt id="arg-top-p">top_p<a class="anchor" aria-label="anchor" href="#arg-top-p"></a></dt>
<dd><p>An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10% probability mass are considered.
OpenAI recommends altering this or temperature but not both. Default is 1.</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Controls randomness: lowering results in less random completions.
As the temperature approaches zero, the model will become deterministic and repetitive. Default is 0.7.</p></dd>


<dt id="arg-decision-description">decision_description<a class="anchor" aria-label="anchor" href="#arg-decision-description"></a></dt>
<dd><p>Logical indicating whether to include detailed descriptions
of decisions. Default is <code>FALSE</code>.</p></dd>


<dt id="arg-assistant-name-assistant-description-assistant-instructions">assistant_name, assistant_description, assistant_instructions<a class="anchor" aria-label="anchor" href="#arg-assistant-name-assistant-description-assistant-instructions"></a></dt>
<dd><p>Configuration for the OpenAI assistant.</p></dd>


<dt id="arg-messages">messages<a class="anchor" aria-label="anchor" href="#arg-messages"></a></dt>
<dd><p>Logical indicating whether to print messages embedded in the function.
Default is <code>TRUE</code>.</p></dd>


<dt id="arg-reps">reps<a class="anchor" aria-label="anchor" href="#arg-reps"></a></dt>
<dd><p>Numerical value indicating the number of times the same
question should be sent to OpenAI's API models. This can be useful to test consistency
between answers. Default is <code>1</code>.</p></dd>


<dt id="arg-max-tries-max-seconds">max_tries, max_seconds<a class="anchor" aria-label="anchor" href="#arg-max-tries-max-seconds"></a></dt>
<dd><p>Cap the maximum number of attempts with
<code>max_tries</code> or the total elapsed time from the first request with
<code>max_seconds</code>. If neither option is supplied, it will not retry.</p></dd>


<dt id="arg-time-info">time_info<a class="anchor" aria-label="anchor" href="#arg-time-info"></a></dt>
<dd><p>Logical indicating whether the run time of each
request/question should be included in the data. Default = <code>TRUE</code>.</p></dd>


<dt id="arg-token-info">token_info<a class="anchor" aria-label="anchor" href="#arg-token-info"></a></dt>
<dd><p>Logical indicating whether the number of prompt and completion tokens
per request should be included in the output data. Default = <code>TRUE</code>.</p></dd>


<dt id="arg-is-transient">is_transient<a class="anchor" aria-label="anchor" href="#arg-is-transient"></a></dt>
<dd><p>A predicate function that takes a single argument
(the response) and returns <code>TRUE</code> or <code>FALSE</code> specifying whether or not
the response represents a transient error.</p></dd>


<dt id="arg-backoff">backoff<a class="anchor" aria-label="anchor" href="#arg-backoff"></a></dt>
<dd><p>A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait.</p></dd>


<dt id="arg-after">after<a class="anchor" aria-label="anchor" href="#arg-after"></a></dt>
<dd><p>A function that takes a single argument (the response) and
returns either a number of seconds to wait or <code>NULL</code>, which indicates
that the <code>backoff</code> strategy should be used instead.</p></dd>


<dt id="arg-rpm">rpm<a class="anchor" aria-label="anchor" href="#arg-rpm"></a></dt>
<dd><p>Numerical value indicating the number of requests per minute (rpm)
available for the specified api key.</p></dd>


<dt id="arg-seed-par">seed_par<a class="anchor" aria-label="anchor" href="#arg-seed-par"></a></dt>
<dd><p>Numerical value for a seed to ensure that proper,
parallel-safe random numbers are produced.</p></dd>


<dt id="arg-progress">progress<a class="anchor" aria-label="anchor" href="#arg-progress"></a></dt>
<dd><p>Logical indicating whether a progress bar should be shown when running
the screening in parallel. Default is <code>TRUE</code>.</p></dd>


<dt id="arg-incl-cutoff-upper">incl_cutoff_upper<a class="anchor" aria-label="anchor" href="#arg-incl-cutoff-upper"></a></dt>
<dd><p>Numerical value indicating the probability threshold
for which a study should be included. Default is 0.5.</p></dd>


<dt id="arg-incl-cutoff-lower">incl_cutoff_lower<a class="anchor" aria-label="anchor" href="#arg-incl-cutoff-lower"></a></dt>
<dd><p>Numerical value indicating the probability threshold
above which studies should be checked by a human. Default is 0.4.</p></dd>


<dt id="arg-force">force<a class="anchor" aria-label="anchor" href="#arg-force"></a></dt>
<dd><p>Logical argument indicating whether to force the function to use more than
10 iterations or certain models. Default is <code>FALSE</code>.</p></dd>


<dt id="arg-sleep-time">sleep_time<a class="anchor" aria-label="anchor" href="#arg-sleep-time"></a></dt>
<dd><p>Time in seconds to wait between checking run status. Default is 8.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Further arguments to pass to the request body.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>An object of class <code>gpt_ftscreen</code> containing:</p>
<dl><dt>answer_data</dt>
<dd><p>Row per (file × prompt × repetition × model) with decisions and cached supplementary value.</p></dd>

<dt>answer_data_aggregated</dt>
<dd><p>Aggregated inclusion probabilities (present when multiple prompts, models, or reps).</p></dd>

<dt>error_data</dt>
<dd><p>Rows where a decision could not be derived (if any).</p></dd>

<dt>run_date</dt>
<dd><p>Date of execution.</p></dd>

<dt>n_files, n_prompts, n_models, n_combinations, n_runs</dt>
<dd><p>Processing counts.</p></dd>

<dt>price_dollar</dt>
<dd><p>Total estimated cost of the screening (in dollars) based on actual token usage.</p></dd>

<dt>price_data</dt>
<dd><p>A tibble with detailed price breakdown per prompt/model/iteration.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="note">Note<a class="anchor" aria-label="anchor" href="#note"></a></h2>
    <p>The <code>answer_data_aggregated</code> data (only present when reps &gt; 1) contains the following variables:</p><table class="table table"><tr><td><b>title</b></td><td><code>character</code></td><td>The filename of the screened document.</td></tr><tr><td><b>model</b></td><td><code>character</code></td><td>The specific model used.</td></tr><tr><td><b>promptid</b></td><td><code>integer</code></td><td>The prompt ID.</td></tr><tr><td><b>prompt</b></td><td><code>character</code></td><td>The prompt used for screening.</td></tr><tr><td><b>incl_p</b></td><td><code>numeric</code></td><td>The probability of inclusion across repeated responses.</td></tr><tr><td><b>final_decision_gpt</b></td><td><code>character</code></td><td>The final decision: 'Include', 'Exclude', or 'Check'.</td></tr><tr><td><b>final_decision_gpt_num</b></td><td><code>integer</code></td><td>The final numeric decision: 1 for include/check, 0 for exclude.</td></tr><tr><td><b>reps</b></td><td><code>integer</code></td><td>The number of repetitions for the question.</td></tr><tr><td><b>n_mis_answers</b></td><td><code>integer</code></td><td>The number of missing responses.</td></tr><tr><td><b>supplementary</b></td><td><code>character</code></td><td>Indicates if supplementary material was detected ('yes'/'no').</td></tr><tr><td><b>longest_answer</b></td><td><code>character</code></td><td>The longest detailed description from responses (if <code>decision_description = TRUE</code>).</td></tr></table><p><br>
The <code>answer_data</code> data contains the following variables:</p><table class="table table"><tr><td><b>studyid</b></td><td><code>integer</code></td><td>The study ID of the file.</td></tr><tr><td><b>title</b></td><td><code>character</code></td><td>The filename of the screened document.</td></tr><tr><td><b>promptid</b></td><td><code>integer</code></td><td>The prompt ID.</td></tr><tr><td><b>prompt</b></td><td><code>character</code></td><td>The prompt used for screening.</td></tr><tr><td><b>model</b></td><td><code>character</code></td><td>The specific model used.</td></tr><tr><td><b>iterations</b></td><td><code>numeric</code></td><td>The repetition number for the question.</td></tr><tr><td><b>decision_gpt</b></td><td><code>character</code></td><td>The raw decision from the model ('1', '0', or '1.1').</td></tr><tr><td><b>detailed_description</b></td><td><code>character</code></td><td>A detailed description of the decision (if <code>decision_description = TRUE</code>).</td></tr><tr><td><b>supplementary</b></td><td><code>character</code></td><td>Indicates if supplementary material was detected ('yes'/'no').</td></tr><tr><td><b>decision_binary</b></td><td><code>integer</code></td><td>The binary decision (1 for inclusion/uncertainty, 0 for exclusion).</td></tr><tr><td><b>run_time</b></td><td><code>numeric</code></td><td>The time taken for the request.</td></tr><tr><td><b>prompt_tokens</b></td><td><code>integer</code></td><td>The number of prompt tokens used.</td></tr><tr><td><b>completion_tokens</b></td><td><code>integer</code></td><td>The number of completion tokens used.</td></tr></table></div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">set_api_key</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">file_path</span> <span class="op">&lt;-</span> <span class="st">"path/to/your/full_text_files"</span></span></span>
<span class="r-in"><span><span class="va">protocol_file</span> <span class="op">&lt;-</span> <span class="st">"path/to/your/protocol_file.txt"</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># --- Run screening using the protocol file ---</span></span></span>
<span class="r-in"><span><span class="va">result_protocol</span> <span class="op">&lt;-</span> <span class="fu">ftscreen</span><span class="op">(</span></span></span>
<span class="r-in"><span>  file_path <span class="op">=</span> <span class="va">file_path</span>,</span></span>
<span class="r-in"><span>  protocol_file_path <span class="op">=</span> <span class="va">protocol_file</span>,</span></span>
<span class="r-in"><span>  vector_stores_name <span class="op">=</span> <span class="st">"TestFTScreenProtocol"</span>,</span></span>
<span class="r-in"><span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>,</span></span>
<span class="r-in"><span>  decision_description <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  reps <span class="op">=</span> <span class="fl">1</span>,</span></span>
<span class="r-in"><span>  assistant_instructions <span class="op">=</span> <span class="st">"</span></span></span>
<span class="r-in"><span><span class="st">  You are a helpful agent that reviews files to determine their relevance based on specific protocols.</span></span></span>
<span class="r-in"><span><span class="st"></span></span></span>
<span class="r-in"><span><span class="st">  CRITICAL: You must follow this EXACT two-step process:</span></span></span>
<span class="r-in"><span><span class="st"></span></span></span>
<span class="r-in"><span><span class="st">  STEP 1: SUPPLEMENTARY CHECK ONLY</span></span></span>
<span class="r-in"><span><span class="st">  - Use the supplementary_check function to identify if the text contains references to</span></span></span>
<span class="r-in"><span><span class="st">    supplementary materials, appendices, or additional information.</span></span></span>
<span class="r-in"><span><span class="st">  - This step is ONLY about identifying supplementary content - do NOT make any inclusion decisions here.</span></span></span>
<span class="r-in"><span><span class="st"></span></span></span>
<span class="r-in"><span><span class="st">  STEP 2: PROTOCOL EVALUATION AND INCLUSION DECISION</span></span></span>
<span class="r-in"><span><span class="st">  - Carefully review the provided protocol criteria.</span></span></span>
<span class="r-in"><span><span class="st">  - Evaluate the study against ALL relevant criteria in the protocol.</span></span></span>
<span class="r-in"><span><span class="st">  - Base your decision solely on whether the study meets the protocol criteria.</span></span></span>
<span class="r-in"><span><span class="st"></span></span></span>
<span class="r-in"><span><span class="st">  IMPORTANT REMINDERS:</span></span></span>
<span class="r-in"><span><span class="st">  - The supplementary check is separate from the inclusion decision.</span></span></span>
<span class="r-in"><span><span class="st">  - If the study meets the protocol criteria, then it should be included in further studies.</span></span></span>
<span class="r-in"><span><span class="st">  - Be explicit about whether the study should be included or excluded based on the protocol evaluation.</span></span></span>
<span class="r-in"><span><span class="st">  - Provide detailed reasoning for your decision when detailed_description is enabled"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">result_protocol</span><span class="op">$</span><span class="va">answer_data</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># --- Run screening using traditional prompts ---</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"</span></span></span>
<span class="r-in"><span><span class="st">Does this study focus on an intervention aimed at improving children's language, </span></span></span>
<span class="r-in"><span><span class="st">reading/literacy, or mathematical skills?</span></span></span>
<span class="r-in"><span><span class="st">"</span>,</span></span>
<span class="r-in"><span><span class="st">"Is this study written in English?"</span>,</span></span>
<span class="r-in"><span><span class="st">"Does this study involve children aged 3 to 4 years old?"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">result_prompts</span> <span class="op">&lt;-</span> <span class="fu">ftscreen</span><span class="op">(</span></span></span>
<span class="r-in"><span>  file_path <span class="op">=</span> <span class="va">file_path</span>,</span></span>
<span class="r-in"><span>  prompt <span class="op">=</span> <span class="va">prompts</span>,</span></span>
<span class="r-in"><span>  vector_stores_name <span class="op">=</span> <span class="st">"TestFTScreenPrompts"</span>,</span></span>
<span class="r-in"><span>  model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span>,</span></span>
<span class="r-in"><span>  decision_description <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>  reps <span class="op">=</span> <span class="fl">1</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">result_prompts</span><span class="op">$</span><span class="va">answer_data</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Mikkel H. Vembye, Thomas Olsen.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

